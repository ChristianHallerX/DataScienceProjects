{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-Way Sentiment Analysis for Twitter Tweets\n",
    "\n",
    "### Overview\n",
    "\n",
    "This NLP project implements a three-way polarity (positive, negative, neutral) classification system for tweets, *without* using NLTK's in-built sentiment analysis engine. \n",
    "\n",
    "Based on Twitter tweets, the project will first run (1) preprocessing, then  compares the accuracy of (2) Dummy, Decision Tree, and Logistic Regression Classifier, (3) Polarity Lexicons (both built-in and manual), and (4) a combination of Logistic Regression with Polarity Lexicon in predictiing the sentiment polarity.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "- training.json: This file contains ~15k raw tweets, along with their polarity labels (1 = positive, 0 = neutral, -1 = negative), which is used for training.\n",
    "- develop.json: In the same format as training.json. File contains a smaller set of tweets used to test the predictions.\n",
    "\n",
    "## Preprocessing Steps\n",
    "\n",
    "First, extract the tweets from the json file, read each line and store the tweets, labels in separate lists.\n",
    "\n",
    "Then for the preprocessing the steps are:\n",
    "\n",
    "- segment tweets into sentences using an NTLK segmenter\n",
    "- tokenize the sentences using an NLTK tokenizer\n",
    "- lowercase all the words\n",
    "- remove twitter usernames beginning with @ using regex\n",
    "- remove URLs starting with http using regex\n",
    "- process hashtags: tokenize hashtags, break down multi-word hashtags using a MaxMatch algorithm and NLTK word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ChristianV700\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ChristianV700\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\ChristianV700\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\ChristianV700\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\ChristianV700\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download NLTK built-in vocabulary\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('word2vec_sample')\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preprocessing\n",
    "\n",
    "### Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "dictionary = set(nltk.corpus.words.words()) #Save words vocabulary in variable. To be used for MaxMatch\n",
    "\n",
    "#Function to lemmatize word | Used during maxmatch\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "#Function to implement the maxmatch algorithm for multi-word hashtags\n",
    "def maxmatch(word,dictionary):\n",
    "    if not word:\n",
    "        return []\n",
    "    for i in range(len(word),1,-1):\n",
    "        first = word[0:i]\n",
    "        rem = word[i:]\n",
    "        if lemmatize(first).lower() in dictionary: #Important to lowercase lemmatized words before comparing in dictionary. \n",
    "            return [first] + maxmatch(rem,dictionary)\n",
    "    first = word[0:1]\n",
    "    rem = word[1:]\n",
    "    return [first] + maxmatch(rem,dictionary)\n",
    "\n",
    "#Function to preprocess a single tweet\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    tweet = re.sub(\"@\\w+\",\"\",tweet).strip()\n",
    "    tweet = re.sub(\"http\\S+\",\"\",tweet).strip()\n",
    "    hashtags = re.findall(\"#\\w+\",tweet)\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"#\\w+\",\"\",tweet).strip() \n",
    "    \n",
    "    hashtag_tokens = [] #Separate list for hashtags\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        hashtag_tokens.append(maxmatch(hashtag[1:],dictionary))        \n",
    "    \n",
    "    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    segmented_sentences = segmenter.tokenize(tweet)\n",
    "    \n",
    "    #General tokenization\n",
    "    processed_tweet = []\n",
    "    \n",
    "    word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "    for sentence in segmented_sentences:\n",
    "        tokenized_sentence = word_tokenizer.tokenize(sentence.strip())\n",
    "        processed_tweet.append(tokenized_sentence)\n",
    "    \n",
    "    #Processing the hashtags only when they exist in a tweet\n",
    "    if hashtag_tokens:\n",
    "        for tag_token in hashtag_tokens:\n",
    "            processed_tweet.append(tag_token)\n",
    "    \n",
    "    return processed_tweet\n",
    "    \n",
    "#Function that takes in a file, and passes each tweet to the preprocessor\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does MaxMatch algorithm disentangle a tweet's hashtags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'can']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1\n",
    "maxmatch('wecan',dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cases', 'tu', 'd', 'y']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "maxmatch('casestudy',dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example shows how maxmatch incorrectly breks down the word 'casestudy', by returning 'cases', instead of 'case' for the first iteration. This is because it _greedily_ extract 'cases' first.\n",
    "\n",
    "For improvement, count the number of successful matches in the result of the maxmatch process, and return the one with the highest successful match count.\n",
    "\n",
    "### Start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the basic preprocessing module and capturing the data\n",
    "train_data = preprocess_file(f'{os.getcwd()}\\\\data\\\\sentiment\\\\training.json')\n",
    "train_tweets = train_data[0]\n",
    "train_labels = train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first pre-processed tweets of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['dear', 'the', 'newooffice', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'lync', 'update', '?'], ['c', \"'\", 'mon', '.']], [['how', 'about', 'you', 'make', 'a', 'system', 'that', 'doesn', \"'\", 't', 'eat', 'my', 'friggin', 'discs', '.'], ['this', 'is', 'the', '2nd', 'time', 'this', 'has', 'happened', 'and', 'i', 'am', 'so', 'sick', 'of', 'it', '!']]]\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Helper Function:\n",
    "\n",
    "A simple script to that runs the preprocessing module on a few tweets and prints the original and processed results side by side if it detects a multi-word hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original Tweet: If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "Processed tweet: [['if', 'i', 'make', 'a', 'game', 'as', 'a', 'universal', 'app', '.'], ['will', 'owners', 'be', 'able', 'to', 'download', 'and', 'play', 'it', 'in', 'november', '?'], ['windows', '1', '0'], ['x', 'box', 'one']]\n",
      "\n",
      "2. Original Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
      "Processed tweet: [['microsoft', ',', 'i', 'may', 'not', 'prefer', 'your', 'gaming', 'branch', 'of', 'business', '.'], ['but', ',', 'you', 'do', 'make', 'a', 'damn', 'fine', 'operating', 'system', '.'], ['Window', 's', '1', '0']]\n",
      "\n",
      "3. Original Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "Processed tweet: [['i', 'will', 'be', 'downgrading', 'and', 'let', 'be', 'out', 'for', 'almost', 'the', '1st', 'yr', 'b4', 'trying', 'it', 'again', '.'], ['Window', 's', '1', '0'], ['Window', 's', '1', '0', 'fail']]\n",
      "\n",
      "4. Original Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
      "Processed tweet: [['2nd', 'computer', 'with', 'same', 'error', '!!!'], ['guess', 'we', 'will', 'shelve', 'this', 'until', 'sp1', '!'], ['Window', 's', '1', '0', 'fail']]\n",
      "\n",
      "5. Original Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
      "Processed tweet: [['sunday', 'morning', ',', 'quiet', 'day', 'so', 'time', 'to', 'welcome', 'in'], ['Window', 's', '1', '0']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing examples of multi-word hashtags (Doesn't work for multi sentence tweets)\n",
    "f = open(f'{os.getcwd()}\\\\data\\\\sentiment\\\\training.json')\n",
    "\n",
    "count = 1\n",
    "for index,line in enumerate(f):\n",
    "    if count >5:\n",
    "        break\n",
    "    original_tweet = json.loads(line)[\"text\"]\n",
    "    hashtags = re.findall(\"#\\w+\",original_tweet)\n",
    "    if hashtags:\n",
    "        for hashtag in hashtags:\n",
    "            if len(maxmatch(hashtag[1:],dictionary)) > 1:\n",
    "                # If the length of the array returned by the maxmatch function is greater than 1,\n",
    "                # it means that the algorithm has detected a hashtag with more than 1 word inside. \n",
    "                print(str(count) + \". Original Tweet: \" + original_tweet + \"\\nProcessed tweet: \" + str(train_tweets[index]) + \"\\n\")\n",
    "                count += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Our pre-processing module is working as intended.\n",
    "\n",
    "The next step is to convert each processed tweet into a bag-of-words feature dictionary. We'll allow for options to remove stopwords during the process, and also to remove _rare_ words, i.e. words occuring less than n times across the whole training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Identify words appearing less than n times create a dictionary for the whole training set\n",
    "\n",
    "total_train_bow = {}\n",
    "\n",
    "for tweet in train_tweets:\n",
    "    for segment in tweet:\n",
    "        for token in segment:\n",
    "            total_train_bow[token] = total_train_bow.get(token,0) + 1\n",
    "\n",
    "# Convert pre_processed tweets to bag of words feature dictionaries\n",
    "# Allows for options to remove stopwords, and also to remove words occuring less than n times in the whole training set.            \n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n): \n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        if remove_stop_words:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        else:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if n<=0 or total_train_bow[token]>=n:\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there is a function to convert raw tweets to feature dictionaries, run it on training and development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Conversion to feature dictionaries\n",
    "train_set = convert_to_feature_dicts(train_tweets,True,2)\n",
    "\n",
    "dev_data = preprocess_file(f'{os.getcwd()}\\\\data\\\\sentiment\\\\develop.json')\n",
    "\n",
    "dev_set = convert_to_feature_dicts(dev_data[0],False,0)\n",
    "\n",
    "# Conversion to sparse representations\n",
    "training_data = vectorizer.fit_transform(train_set)\n",
    "\n",
    "development_data = vectorizer.transform(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Machine Learning Classification\n",
    "\n",
    "### Dummy Classifier\n",
    "-> picks most frequently occuring class as the output as baseline benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common class baseline accuracy: 0.42044833242208857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# The Dummy Classifier always predicts the most frequent class, as specified in the strategy. \n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(development_data,dev_data[1])\n",
    "dummy_predictions = dummy_clf.predict(development_data)\n",
    "\n",
    "print(\"\\nMost common class baseline accuracy: \" + str(accuracy_score(dev_data[1],dummy_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a three-way classification, the baseline must be below 0.5.\n",
    "\n",
    "### Decision Tree Classifier\n",
    "-> tuned with Grid Search over parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for Decision Tree: {'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 75}\n",
      "\n",
      "Decision Tree Accuracy: 0.48715144887916895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Grid used to test the combinations of parameters\n",
    "tree_param_grid = [\n",
    "    {'criterion':['gini','entropy'], 'min_samples_leaf': [75,100,125,150,175], 'max_features':['sqrt','log2',None],\n",
    "    }\n",
    "]\n",
    "\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(),tree_param_grid,cv=10,scoring='accuracy')\n",
    "\n",
    "tree_clf.fit(training_data,train_data[1])\n",
    "\n",
    "print(\"Optimal parameters for Decision Tree: \" + str(tree_clf.best_params_)) # Print best discovered combination of the parameters\n",
    "\n",
    "tree_predictions = tree_clf.predict(development_data)\n",
    "\n",
    "print(\"\\nDecision Tree Accuracy: \" + str(accuracy_score(dev_data[1],tree_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree classifier scores relatively low, but beetter than the dummy.\n",
    "\n",
    "### Logisitc Regression Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for LR: {'C': 0.0125, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "Logistic Regression Accuracy: 0.49371241115363584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_param_grid = [\n",
    "    {'C':[0.012,0.0125,0.130,0.135,0.14],\n",
    "     'solver':['lbfgs'],'multi_class':['multinomial']\n",
    "    }\n",
    "]\n",
    "\n",
    "log_clf = GridSearchCV(LogisticRegression(max_iter=200),log_param_grid,cv=10,scoring='accuracy')\n",
    "\n",
    "log_clf.fit(training_data,train_data[1])\n",
    "\n",
    "log_predictions = log_clf.predict(development_data)\n",
    "\n",
    "print(\"Optimal parameters for LR: \" + str(log_clf.best_params_))\n",
    "\n",
    "print(\"Logistic Regression Accuracy: \" + str(accuracy_score(dev_data[1],log_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Classifier</th>\n",
    "<th>Approx. Accuracy score (in %)</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Dummy classifier (most common class)</td>\n",
    "<td>42</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Decision Tree classifier</td>\n",
    "<td>48.7</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Logistic Regression classifier</td>\n",
    "<td>49.3</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Polarity Lexicons\n",
    "\n",
    "Integrate external information into the training set in the form of polarity scores for the tweets. \n",
    "\n",
    "Build two automatic lexicons, compare it with NLTK's manually annotated set, and then add that information to our training data.\n",
    "\n",
    "**Lecixon 1** will be built through SentiWordNet. This lexicon contains pre-calculated scores positive, negative and neutral sentiments for some words in WordNet. As this information is arranged in the form of synsets, take the most common polarity across its senses (and take neutral in case of a tie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words: ['associable', 'thoroughly', 'palmatifid', 'probative', 'arable']\n",
      "Negative Words: ['associative', 'hairlessness', 'spastic_bladder', 'unimpeded', 'unbefitting']\n"
     ]
    }
   ],
   "source": [
    "swn_positive = []\n",
    "swn_negative = []\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for synset in wn.all_synsets():      \n",
    "    # count synset polarity for each lemma\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for lemma in synset.lemma_names():\n",
    "        for syns in wn.synsets(lemma):\n",
    "            if get_polarity_type(syns.name())==1:\n",
    "                pos_count+=1\n",
    "            elif get_polarity_type(syns.name())==-1:\n",
    "                neg_count+=1\n",
    "            else:\n",
    "                neutral_count+=1\n",
    "    \n",
    "    if pos_count > neg_count and pos_count >= neutral_count:    # >=neutral as words that are more positive than negative, \n",
    "                                                                # despite being equally neutral might belong to positive list (explain)\n",
    "        swn_positive.append(synset.lemma_names()[0])\n",
    "    elif neg_count > pos_count and neg_count >= neutral_count:\n",
    "        swn_negative.append(synset.lemma_names()[0])       \n",
    "\n",
    "swn_positive = list(set(swn_positive))\n",
    "swn_negative = list(set(swn_negative))\n",
    "\n",
    "print('Positive words: ' + str(random.sample(swn_positive,5)))\n",
    "print('Negative Words: ' + str(random.sample(swn_negative,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity of a synset is defined by the lemma names that were extracted from the synset to get its 'senses'. Next, each of those lemma names were converted to a synset object, which were then passed to the pre-supplied 'get_polarity_type' function. Based on the score returned, the head lemma of the synset object was appended to the positive, negative, or neutral lists. The head lemma was chosen from the lemma_names, as it best represents the synset object.\n",
    "\n",
    "The printed words above are a random sample of positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexicon 2** will use the word2vec (CBOW) vectors included in NLTK. \n",
    "\n",
    "Using a small set of positive and negative seed terms, the next cells calculate the cosine similarity between vectors of seed terms and another word.\n",
    "Then, use Gensim to iterate over words in model.vocab for comparison over seed terms. \n",
    "\n",
    "After calculating the cosine similarity of a word with both the positive and negative terms, calculate their average, after flipping the sign for negative seeds.\n",
    "\n",
    "A threshold of ±0.03 will be used to determine if words are positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words: ['proudly', 'professional', 'handsome', 'locate', 'distinguished']\n",
      "Negative Words: ['millenarianism', 'attacked', 'unsympathetic', 'rots', 'cranky']\n"
     ]
    }
   ],
   "source": [
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample,binary=False)\n",
    "\n",
    "wv_positive = []\n",
    "wv_negative = []\n",
    "\n",
    "for word in model.vocab:\n",
    "    try:\n",
    "        word=word.lower()\n",
    "    \n",
    "        pos_score = 0.0\n",
    "        neg_score = 0.0\n",
    "    \n",
    "        for seed in positive_seeds:\n",
    "            pos_score = pos_score + model.similarity(word,seed)\n",
    "    \n",
    "        for seed in negative_seeds:\n",
    "            neg_score = neg_score + model.similarity(word,seed)\n",
    "        \n",
    "        avg = (pos_score - neg_score)/16 # Total number of seeds is 16\n",
    "    \n",
    "        if avg>0.03:\n",
    "            wv_positive.append(word)\n",
    "        elif avg<-0.03:\n",
    "            wv_negative.append(word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('Positive words: ' + str(random.sample(wv_positive,5)))\n",
    "print('Negative Words: ' + str(random.sample(wv_negative,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the printed samples are randomised.\n",
    "\n",
    "Considering the sample, this looks like a good set of both positive negative words, looking at the samples.\n",
    "\n",
    "See how it compares with NLTK's manually annotated set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Lexicon\n",
    "\n",
    "The lexicon included with NLTK contains a list of positive and negative words.\n",
    "\n",
    "First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicon.\n",
    "Second, only for those words which overlap and are not in the seed set, evaluate the accuracy of with each of the automatic lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words in manual lexicons also present in the automatic lexicon\n",
      "First automatic lexicon: 13.610251878038001\n",
      "Second automatic lexicon: 37.796435410222415\n",
      "\n",
      "Accuracy of lexicons: \n",
      "First automatic lexicon: 84.46389496717724\n",
      "Second automatic lexicon: 98.94159153273226\n"
     ]
    }
   ],
   "source": [
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "# Calculate the percentage of words in the manually annotated lexicon set, that also appear in an automatic lexicon.\n",
    "def get_perc_manual(manual_pos,manual_neg,auto_pos,auto_neg):\n",
    "    return len(set(manual_pos+manual_neg).intersection(set(auto_pos+auto_neg)))/len(manual_pos+manual_neg)*100\n",
    "\n",
    "print(\"Percentage of words in manual lexicons also present in the automatic lexicon\")\n",
    "print(\"First automatic lexicon: \"+ str(get_perc_manual(positive_words,negative_words,swn_positive,swn_negative)))\n",
    "print(\"Second automatic lexicon: \"+ str(get_perc_manual(positive_words,negative_words,wv_positive,wv_negative)))\n",
    "\n",
    "# Calculate the accuracy of words in the automatic lexicon. Assuming that the manual lexicons are accurate, it calculates the percentage of words that occur in both positive and negative (respectively) lists of automatic and manual lexicons.\n",
    "def get_lexicon_accuracy(manual_pos,manual_neg,auto_pos,auto_neg):\n",
    "    common_words = set(manual_pos+manual_neg).intersection(set(auto_pos+auto_neg))-set(negative_seeds)-set(positive_seeds)\n",
    "    return (len(set(manual_pos) & set(auto_pos) & common_words)+len(set(manual_neg) & set(auto_neg) & common_words))/len(common_words)*100\n",
    "\n",
    "print(\"\\nAccuracy of lexicons: \")\n",
    "print(\"First automatic lexicon: \"+ str(get_lexicon_accuracy(positive_words,negative_words,swn_positive,swn_negative)))\n",
    "print(\"Second automatic lexicon: \"+ str(get_lexicon_accuracy(positive_words,negative_words,wv_positive,wv_negative)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second lexicon shares the most common words with the manual lexicon, and has the most accurately classified words, as it uses the most intutive way of creative positive/negative lexicons i.e. by identifying the most similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicons for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used the lexicons for the main classification problem? \n",
    "\n",
    "Let's create a function that calculates a polarity score for a sentence based on a given lexicon. We'll count the positive and negative words that appear in the tweet, and then return a +1 if there are more posiitve words, a -1 if there are more negative words, and a 0 otherwise.\n",
    "\n",
    "We'll then compare the results of the three lexicons on the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual lexicon accuracy: 45.2159650082012\n",
      "First auto lexicon accuracy: 42.3728813559322\n",
      "Second auto lexicon accuracy: 45.16129032258064\n"
     ]
    }
   ],
   "source": [
    "# All lexicons are converted to sets for faster preprocessing\n",
    "manual_pos_set = set(positive_words)\n",
    "manual_neg_set = set(negative_words)\n",
    "\n",
    "syn_pos_set = set(swn_positive)\n",
    "syn_neg_set = set(swn_negative)\n",
    "\n",
    "wordvec_pos_set = set(wv_positive)\n",
    "wordvec_neg_set = set(wv_negative)\n",
    "\n",
    "# Function for polarity score of a sentence based on the frequency of positive or negative words\n",
    "def get_polarity_score(sentence,pos_lexicon,neg_lexicon):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in pos_lexicon:\n",
    "            pos_count+=1\n",
    "        if word in neg_lexicon:\n",
    "            neg_count+=1\n",
    "    if pos_count>neg_count:\n",
    "        return 1\n",
    "    elif neg_count>pos_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Function for score of each tweet and compare against labels of the dataset and calculate/count  accuracy score\n",
    "def data_polarity_accuracy(dataset,datalabels,pos_lexicon,neg_lexicon):\n",
    "    accuracy_count = 0\n",
    "    \n",
    "    for index,tweet in enumerate(dataset):\n",
    "        if datalabels[index]==get_polarity_score([word for sentence in tweet for word in sentence],pos_lexicon,neg_lexicon):\n",
    "            accuracy_count+=1\n",
    "    return (accuracy_count/len(dataset))*100\n",
    "\n",
    "\n",
    "print(\"Manual lexicon accuracy: \" + str(data_polarity_accuracy(dev_data[0],dev_data[1],manual_pos_set,manual_neg_set)))\n",
    "print(\"First auto lexicon accuracy: \" + str(data_polarity_accuracy(dev_data[0],dev_data[1],syn_pos_set,syn_neg_set)))\n",
    "print(\"Second auto lexicon accuracy: \" + str(data_polarity_accuracy(dev_data[0],dev_data[1],wordvec_pos_set,wordvec_neg_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results reflect the quality metric obtained from the previous section, with the manual and second lexicon (word vector) slightly ahead. However, still not as good as a Machine Learning algorithm without the polarity information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Combo: Polarity Lexicon with Logistic Regression\n",
    "\n",
    "Lastly, add the lexicon polarity score as a feature for a classifier. \n",
    "\n",
    "For this, create a modified version of the feature extraction function to integrate the extra *feature* and retrain the logisitc regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_feature_dicts_v2(tweets,manual,first,second,remove_stop_words,n): \n",
    "    feature_dicts = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # Build feature dictionary for tweet\n",
    "        feature_dict = {}\n",
    "        \n",
    "        if remove_stop_words:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        else:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if n<=0 or total_train_bow[token]>=n:\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        if manual == True:\n",
    "            feature_dict['manual_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],manual_pos_set,manual_neg_set)\n",
    "        if first == True:\n",
    "            feature_dict['synset_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],syn_pos_set,syn_neg_set)\n",
    "        if second == True:\n",
    "            feature_dict['wordvec_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],wordvec_pos_set,wordvec_neg_set)\n",
    "\n",
    "        feature_dicts.append(feature_dict)      \n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training set\n",
    "training_set_v2 = convert_to_feature_dicts_v2(train_tweets,True,False,True,True,2)\n",
    "\n",
    "training_data_v2 = vectorizer.fit_transform(training_set_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# convert dev set\n",
    "dev_set_v2 = convert_to_feature_dicts_v2(dev_data[0],True,False,True,False,0)\n",
    "\n",
    "development_data_v2 = vectorizer.transform(dev_set_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.012, multi_class='multinomial')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train logistic regression\n",
    "log_clf_v2 = LogisticRegression(C=0.012,solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "log_clf_v2.fit(training_data_v2,train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression V2 (with polarity scores) Accuracy: 0.5079278294149808\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "log_predictions_v2 = log_clf_v2.predict(development_data_v2)\n",
    "\n",
    "print(\"Logistic Regression V2 (with polarity scores) Accuracy: \" + str(accuracy_score(dev_data[1],log_predictions_v2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though minimal (1.7%), there was improvement in the Logistic Regression Classifier by integrating the polarity data. \n",
    "\n",
    "This concludes the project of building a three-way polarity classifier for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
