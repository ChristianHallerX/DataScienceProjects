{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Way IMDB film database sentiment analysis with Keras LSTM \n",
    "\n",
    "## Project Goals:\n",
    "\n",
    "Analyze 25,000 movie reviews in IMDB if positive (1) or negative (0) sentiment.\n",
    "\n",
    "The data set is provided by preprocessed by Keras with words transcribed to a number-encoded vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading IMDB sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000   # 20000 most common items from vocabulary\n",
    "maxlen = 80   # word length of a review\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Workaround for numpy allow_pickle=False problem with the imdb data set. Downgrade of numpy version -- Does not belong in the normal modeling workflow. Will be fixed in future versions.\n",
    "\n",
    "\n",
    "\n",
    "#!pip install numpy==1.16.2\n",
    "print('installed')\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Train Sequences\n",
      "25000 Test Sequences\n",
      "\n",
      "First item features:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "First item label:  1\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# check the contents of the vectors. These are the encoded words and the positive/negative label.\n",
    "print(len(x_train), 'Train Sequences')\n",
    "print(len(x_test), 'Test Sequences')\n",
    "print('\\nFirst item features: ',x_train[0])\n",
    "print('\\nFirst item label: ',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences (reviews) that are too short and cutting too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen) \n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# note: the output layer has only one single node meaning good (1) or bad (0)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# not so nice model representation\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Run and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristianV700\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.4616 - accuracy: 0.7801 - val_loss: 0.3846 - val_accuracy: 0.8304\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.3035 - accuracy: 0.8761 - val_loss: 0.3788 - val_accuracy: 0.8358\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.2243 - accuracy: 0.9128 - val_loss: 0.4214 - val_accuracy: 0.8285\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.1586 - accuracy: 0.9410 - val_loss: 0.4922 - val_accuracy: 0.8285\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.1151 - accuracy: 0.9583 - val_loss: 0.5734 - val_accuracy: 0.8032\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0884 - accuracy: 0.9687 - val_loss: 0.6184 - val_accuracy: 0.8143\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0637 - accuracy: 0.9798 - val_loss: 0.6796 - val_accuracy: 0.8209\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0480 - accuracy: 0.9834 - val_loss: 0.7612 - val_accuracy: 0.8158\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0347 - accuracy: 0.9882 - val_loss: 0.8117 - val_accuracy: 0.8075\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.0306 - accuracy: 0.9905 - val_loss: 0.8508 - val_accuracy: 0.8090\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.9665 - val_accuracy: 0.8130\n",
      "Epoch 12/15\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9941"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=15,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('Test score: ', score)\n",
    "print('Test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This is a quite good training accuracy.\n",
    "\n",
    "However, the validaton accuracy may still require some improvement (80.5%).\n",
    "\n",
    "LSTMs (RNNs) can be difficult to improve.\n",
    "\n",
    "The correct choice of batch size, loss algorithm, and optimizer is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
