{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drowsiness Detection in Real Time\n",
    "\n",
    "This project is intended to detect drowsiness in real-time video footage, such as a webcam feed.\n",
    "\n",
    "The implemented algorithm will grab webcam footage, detect face/eyes and crop them, and then send them to a CNN (Convolutional Neural Network) model in Keras/TensorFlow that will evaluate the eyes.\n",
    "\n",
    "If closed eyes were predicted for several consecutive frames, a warning chime will sound. When eyes are predicted open again, everything is supposed to go back to normal.\n",
    "\n",
    "## Prerequisites (prepared in advance)\n",
    "\n",
    "- **Dataset creation**. To create the DNN dataset, a script was written that captures eyes from a camera and stored them on local disk. They were labeled \"Open\" or \"Closed\". The data was manually cleaned by removing bad and redundant images. The dataset comprised ~7,000 images of people’s eyes under different lighting conditions. Sorry, this is too big for GitHub.\n",
    "\n",
    "- **Training**. Next, a Deep Neural Network (DNN) model was trained to classify open and closed eyes. See file  <a href=\"https://github.com/ChristianHallerX/DataScienceProjects/blob/master/Data/drowsiness_data/Drowsiness_detection_CNN_pre-train.ipynb \" target=\"_blank\">\"Drowsiness_detection_CNN_pre-train.ipynb\"</a> and the the final weights and model architecture file \"cnnCat2.h5\".\n",
    "\n",
    "- **Object Detection.** The Haar Cascade <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\" target=\"_blank\">(example in OpenCV)</a> finds landmarks in image frames and cuts them out. The Haar Cascade algorithm produces similar results as the newer YOLO object detection using image convolution filters but instead uses kernel computation. Kernel computation is very similar to YOLO's convolution filters. Different kernels are tuned for different landmarks and can be loaded as needed. The \"cascade\" part of the algorithm is used to narrow down features to the wanted areas and save computation. The kernels we will use here are set to detect eyes and face. Many pre-tuned kernels are shared by OpenCV for different purposes and objects. <a href=\"https://github.com/opencv/opencv/tree/master/data/haarcascades\" target=\"_blank\">Link to OpenCV's kernel repo</a>.\n",
    "\n",
    "Data Repository: https://github.com/ChristianHallerX/DataScienceProjects/tree/master/Data/drowsiness_data\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from pygame import mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Alarm setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Channel at 0x1e14fd8de10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer.init()\n",
    "sound = mixer.Sound(f'{os.getcwd()}\\\\Data\\\\drowsiness_data\\\\Electronic_Chime-KevanGC-495939803.wav')\n",
    "\n",
    "# test the alarm sound\n",
    "sound.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load OpenCV Haar kernels used to detect facial landmarks\n",
    "The “haar cascade\" xml files are needed to detect objects from the image. In this case, the kernels are tuned to detect the face and eyes of the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.CascadeClassifier(f'{os.getcwd()}\\\\Data\\\\drowsiness_data\\\\haarcascade_frontalface_alt.xml')\n",
    "leye = cv2.CascadeClassifier(f'{os.getcwd()}\\\\Data\\\\drowsiness_data\\\\haarcascade_lefteye_2splits.xml')\n",
    "reye = cv2.CascadeClassifier(f'{os.getcwd()}\\\\Data\\\\drowsiness_data\\\\haarcascade_righteye_2splits.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the CNN model and start video stream\n",
    "\n",
    "The inference happens in an infinite while-loop that grabs a frame from the webcam feed, identifies the face and eyes, runs a classifier on the eyes, and then calculates a \"drowsiness-score\".\n",
    "\n",
    "We can select a score threshold when a sound will be triggered.\n",
    "\n",
    "Effectively, the score will increase with closed eyes and is a measure for *how long* the eyes were closed.\n",
    "\n",
    "The threshold that has to be reached before sounding the alarm prevents the model from predicting simple blinking as drowsiness. Futhermore, both eyes need to be closed at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video stream open.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(f'{os.getcwd()}\\\\Data\\\\drowsiness_data\\\\cnncat2.h5')\n",
    "path = os.getcwd()\n",
    "\n",
    "# Capture accesses the video feed. The \"0\" is the number of your video device, in case you have multiple.\n",
    "cap = cv2.VideoCapture(0)\n",
    "if cap.isOpened() == True:\n",
    "    print(\"Video stream open.\")\n",
    "else:\n",
    "    print(\"Problem opening video stream.\")\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "\n",
    "# starting value for closed-eyes inferences\n",
    "score = 0\n",
    "threshold = 6\n",
    "thicc = 2\n",
    "rpred = [99]\n",
    "lpred = [99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start prediction cycle\n",
    "When executing the next cell, a window with the video capture will pop up. \n",
    "\n",
    "Quit by hitting 'q'. To re-start, open the video capture again (execute cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite loop of capturing, infering, and scoring\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    height,width = frame.shape[:2]\n",
    "    \n",
    "    # convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Haar Cascade object detection in OpenCV to gray frame\n",
    "    faces = face.detectMultiScale(gray,minNeighbors=5,scaleFactor=1.1,minSize=(25,25))\n",
    "    left_eye = leye.detectMultiScale(gray)\n",
    "    right_eye =  reye.detectMultiScale(gray)\n",
    "    \n",
    "    # draw black bars top and bottom\n",
    "    cv2.rectangle(frame, (0,height-50) , (width,height) , (0,0,0) , thickness=cv2.FILLED )\n",
    "    \n",
    "    # draw face box\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y) , (x+w,y+h) , (255,255,255) , 1 )\n",
    "\n",
    "    # take detected RIGHT eye, preprocess and make CNN prediction \n",
    "    for (x,y,w,h) in right_eye:\n",
    "        r_eye = frame[y:y+h,x:x+w]\n",
    "        r_eye = cv2.cvtColor(r_eye,cv2.COLOR_BGR2GRAY)\n",
    "        r_eye = cv2.resize(r_eye,(24,24))\n",
    "        r_eye = r_eye/255\n",
    "        r_eye =  r_eye.reshape(24,24,-1)\n",
    "        r_eye = np.expand_dims(r_eye,axis=0)\n",
    "        rpred = model.predict_classes(r_eye)\n",
    "        break\n",
    "\n",
    "    # take detected LEFT eye, preprocess and make CNN prediction \n",
    "    for (x,y,w,h) in left_eye:\n",
    "        l_eye = frame[y:y+h,x:x+w]\n",
    "        l_eye = cv2.cvtColor(l_eye,cv2.COLOR_BGR2GRAY)  \n",
    "        l_eye = cv2.resize(l_eye,(24,24))\n",
    "        l_eye = l_eye/255\n",
    "        l_eye =l_eye.reshape(24,24,-1)\n",
    "        l_eye = np.expand_dims(l_eye,axis=0)\n",
    "        lpred = model.predict_classes(l_eye)\n",
    "        break\n",
    "\n",
    "\n",
    "    # labeling for frame, if BOTH eyes close, print CLOSED and adding/subtracting from score tally\n",
    "    if(rpred[0]==0 and lpred[0]==0):\n",
    "        score += 1\n",
    "        cv2.putText(frame,\"Closed\",(10,height-20), font, 1,(255,255,255),1,cv2.LINE_AA)\n",
    "        # prevent a runaway score beyond threshold\n",
    "        if score > threshold + 1:\n",
    "            score = threshold\n",
    "    else:\n",
    "        score -= 1\n",
    "        cv2.putText(frame,\"Open\",(10,height-20), font, 1,(255,255,255),1,cv2.LINE_AA)\n",
    "    \n",
    "    # SCORE HANDLING\n",
    "    # print current score to screen\n",
    "    if(score < 0):\n",
    "        score = 0   \n",
    "    cv2.putText(frame,'Drowsiness Score:'+str(score),(100,height-20), font, 1,(255,255,255),1,cv2.LINE_AA)\n",
    "    \n",
    "    # threshold exceedance\n",
    "    if(score>threshold):\n",
    "        # save a frame when threshold exceeded and play sound\n",
    "        cv2.imwrite(os.path.join(path,'closed_eyes_screencap.jpg'),frame)\n",
    "        try:\n",
    "            sound.play()\n",
    "        except:  # isplaying = False\n",
    "            pass\n",
    "        \n",
    "        # add red box as warning signal and make box thicker\n",
    "        if(thicc<16):\n",
    "            thicc += 2\n",
    "        # make box thinner again, to give it a pulsating appearance\n",
    "        else:\n",
    "            thicc -= 2\n",
    "            if(thicc<2):\n",
    "                thicc=2\n",
    "        cv2.rectangle(frame, (0,0), (width,height), (0,0,255), thickness=thicc)\n",
    "        \n",
    "    # draw frame with all the stuff with have added\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    # break the infinite loop when pressing q\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# close stream capture and close window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
